"""CUDA inference runner for {{ model_name }}."""

import time
import torch
from .model import Model


class {{ runner_class }}:
    def __init__(self, node):
        self.node = node
        self.logger = node.get_logger()
        self.logger.info('{{ runner_class }}: initializing...')

        self.logger.info('{{ runner_class }}: detecting CUDA device...')
        self.device = torch.device('cuda:{{ device_id }}')
        self.logger.info(f'{{ runner_class }}: using device {self.device}')
        self.logger.info(f'{{ runner_class }}: CUDA available={torch.cuda.is_available()}, '
                         f'device_count={torch.cuda.device_count()}')
        if torch.cuda.is_available():
            self.logger.info(f'{{ runner_class }}: GPU name={torch.cuda.get_device_name({{ device_id }})}')

        self.logger.info('{{ runner_class }}: creating model instance...')
        t0 = time.monotonic()
        variant = node.get_parameter('variant').value
        self.model = Model(node, variant)
        self.logger.info(f'{{ runner_class }}: model instance created in {time.monotonic() - t0:.2f}s')

        self.logger.info('{{ runner_class }}: loading weights to device...')
        t0 = time.monotonic()
        self.model.load(self.device)
        self.logger.info(f'{{ runner_class }}: weights loaded in {time.monotonic() - t0:.2f}s')

{% if model_family != "ultralytics" %}
        # torch.compile: fuses ops and eliminates Python overhead after warmup.
        self._use_compile = node.declare_parameter('use_torch_compile', True).value
        if self._use_compile:
            self.logger.info('{{ runner_class }}: applying torch.compile to forward()...')
            t0 = time.monotonic()
            self.model.forward = torch.compile(self.model.forward)
            self.logger.info(f'{{ runner_class }}: torch.compile applied in {time.monotonic() - t0:.2f}s '
                             '(first inference will trigger compilation)')
        else:
            self.logger.info('{{ runner_class }}: torch.compile disabled via parameter')
{% endif %}

        self.logger.info('{{ runner_class }}: ready.')

{% if model_family == "ultralytics" %}
    def infer(self, image):
        """Run inference using Ultralytics API (handles pre/post internally)."""
        raw = self.model.predict(image)
        return self.model.postprocess(raw)
{% elif has_text_input %}
    def infer(self, image, text=None):
        """Run inference with image and text input."""
        inputs = self.model.preprocess(image, text=text)
        with torch.no_grad():
            outputs = self.model.forward(inputs)
        return self.model.postprocess(outputs, original_size=image.shape[:2])
{% else %}
    def infer(self, image):
        """Run inference: preprocess -> forward -> postprocess."""
        inputs = self.model.preprocess(image)
        with torch.no_grad():
            outputs = self.model.forward(inputs)
        return self.model.postprocess(outputs, original_size=image.shape[:2])
{% endif %}
